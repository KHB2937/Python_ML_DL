{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zihvvan/DeepLearning/blob/main/zihvvan/ch09_01%2B02%2B03_%EB%B3%B5%EC%8A%B5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 나는 딥러닝 복습을 할 거야 너는 딥러닝 개념과 파이토치 사용법에 대해서 알려줘"
      ],
      "metadata": {
        "id": "n8pcqf1eCgZY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 파이토치에서의 텐서의 개념과 사용방법을 알려줘\n",
        "\n"
      ],
      "metadata": {
        "id": "tboMFCu7CkRo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "파이토치에서 Tensor는 NumPy 배열과 유사한 다차원 배열을 나타내는 클래스입니다. Tensor는 다양한 딥러닝 작업에 사용됩니다. 예를 들어, 이미지 분류, 자연어 처리 및 강화 학습과 같은 분야에서 Tensor는 입력 데이터, 가중치, 손실 등을 표현하는 데 사용됩니다.\n",
        "\n",
        "Tensor의 가장 중요한 특징 중 하나는 그래디언트(gradient)를 계산할 수 있다는 것입니다. 그래디언트는 손실 함수를 최소화하는 방향으로 가중치를 업데이트하는 데 사용됩니다. 이를 통해 모델이 자동으로 역전파(backpropagation)를 수행하여 가중치를 학습할 수 있습니다."
      ],
      "metadata": {
        "id": "6WH5HpTvCmC4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 텐서에서의 랭크 개념을 설명해줘"
      ],
      "metadata": {
        "id": "wCXsyFv7Cn-A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tensor의 랭크(Rank)는 Tensor의 차원 수를 나타내는 개념입니다. 즉, Tensor가 몇 차원인지를 나타냅니다. Tensor의 랭크는 Tensor의 축(axis) 수와 동일합니다."
      ],
      "metadata": {
        "id": "wtVqEbS0CpiY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# 1차원 Tensor (벡터)\n",
        "a = torch.tensor([1, 2, 3])\n",
        "\n",
        "# 2차원 Tensor (행렬)\n",
        "b = torch.tensor([[1, 2], [3, 4]])\n",
        "\n",
        "# 3차원 Tensor (큐브)\n",
        "c = torch.tensor([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])"
      ],
      "metadata": {
        "id": "0u85iepoCsxP"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 0차원 스칼라에 대한 설명을 추가해줘"
      ],
      "metadata": {
        "id": "QpLgHRGACrZQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "0차원 스칼라는 크기(size)가 없는 Tensor로, 하나의 값만 가지는 Tensor를 말합니다. 즉, 스칼라는 Tensor의 가장 작은 단위입니다. 스칼라 Tensor는 다음과 같이 생성할 수 있습니다."
      ],
      "metadata": {
        "id": "w-_89oWSCu7Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "위의 코드에서 x는 크기가 없는 0차원 Tensor입니다. 즉, 스칼라 Tensor입니다. 스칼라 Tensor는 일반적으로 상수(constant)를 저장하는 데 사용됩니다. 예를 들어, 신경망의 바이어스(bias)와 같은 일부 파라미터는 스칼라 Tensor로 나타낼 수 있습니다.\n",
        "\n",
        "스칼라 Tensor는 기본적으로 일반적인 수학 연산에 사용할 수 있습니다. 예를 들어, 덧셈, 뺄셈, 곱셈, 나눗셈 등의 연산은 스칼라 Tensor에서도 동일하게 적용됩니다."
      ],
      "metadata": {
        "id": "_2lXTJgGCyMA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tensor에서 스칼라 값을 추출하는 방법은 여러 가지가 있습니다. 다음은 몇 가지 방법입니다.\n",
        "\n",
        "### item() 메서드 사용\n",
        "Tensor의 값이 하나일 경우, item() 메서드를 사용하여 스칼라 값을 추출할 수 있습니다.\n",
        "\n",
        "```\n",
        "import torch\n",
        "\n",
        "x = torch.tensor(3.14)\n",
        "y = x.item()\n",
        "print(y)  # 출력: 3.140000104904175\n",
        "```\n",
        "위의 코드에서 item() 메서드는 Tensor에서 스칼라 값을 추출하여 Python의 float 타입으로 반환합니다.\n",
        "\n",
        "### detach() 메서드 및 NumPy 변환 사용\n",
        "Tensor에서 스칼라 값을 추출하기 위해 다음과 같이 detach() 메서드를 사용하여 Tensor를 분리한 후, NumPy 변환 함수를 사용하여 스칼라 값을 추출할 수 있습니다.\n",
        "\n",
        "```\n",
        "import torch\n",
        "\n",
        "x = torch.tensor(3.14)\n",
        "y = x.detach().numpy()[0]\n",
        "print(y)  # 출력: 3.140000104904175\n",
        "```\n",
        "위의 코드에서 detach() 메서드는 Tensor를 분리하여 새로운 Tensor를 반환하고, numpy() 메서드는 Tensor를 NumPy 배열로 변환합니다. NumPy 배열에서 첫 번째 값을 추출하면 스칼라 값을 얻을 수 있습니다.\n",
        "\n",
        "### numpy() 메서드 사용\n",
        "Tensor에서 스칼라 값을 추출하기 위해 다음과 같이 numpy() 메서드를 사용하여 Tensor를 NumPy 배열로 변환한 후, 인덱싱을 사용하여 스칼라 값을 추출할 수 있습니다.\n",
        "\n",
        "```\n",
        "import torch\n",
        "\n",
        "x = torch.tensor(3.14)\n",
        "y = x.numpy()[0]\n",
        "print(y)  # 출력: 3.140000104904175\n",
        "```\n",
        "\n",
        "위의 코드에서 numpy() 메서드는 Tensor를 NumPy 배열로 변환합니다. NumPy 배열에서 첫 번째 값을 추출하면 스칼라 값을 얻을 수 있습니다.\n",
        "\n",
        "위의 세 가지 방법 중 어떤 것을 사용해도 스칼라 값을 추출할 수 있습니다. 단, Tensor가 여러 개의 값을 가지는 경우, item() 메서드를 사용할 수 없으며 다른 방법을 사용해야 합니다."
      ],
      "metadata": {
        "id": "VDutGUyRC0GY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 텐서의 차원과 사이즈를 표현하는 방법에 대해서 알려줘"
      ],
      "metadata": {
        "id": "2YMe0pMEHO9u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 차원(Dimension) 표현\n",
        "Tensor의 차원은 Tensor의 축(axis) 수를 의미합니다. 파이썬에서는 리스트(list)와 유사한 대괄호(square bracket)를 사용하여 Tensor의 차원을 나타냅니다. 예를 들어, 다음과 같은 2차원 Tensor는 [2, 3]으로 표현됩니다.\n",
        "\n",
        "```\n",
        "import torch\n",
        "\n",
        "x = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
        "print(x.shape)  # 출력: torch.Size([2, 3])\n",
        "```\n",
        "위의 코드에서 shape 속성은 Tensor의 모양(shape)을 나타내는 튜플(tuple)을 반환합니다. 이 튜플의 길이는 Tensor의 차원 수와 동일합니다. 위의 Tensor의 shape은 (2, 3)이므로, 이 Tensor는 2차원이며, 첫 번째 차원의 크기는 2이고, 두 번째 차원의 크기는 3입니다.\n",
        "\n",
        "### 사이즈(Size) 표현\n",
        "Tensor의 사이즈는 Tensor의 각 차원의 크기를 의미합니다. 파이썬에서는 torch.Size 클래스를 사용하여 Tensor의 사이즈를 표현합니다. 예를 들어, 다음과 같은 2차원 Tensor의 사이즈는 torch.Size([2, 3])으로 표현됩니다.\n",
        "\n",
        "```\n",
        "import torch\n",
        "\n",
        "x = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
        "print(x.size())  # 출력: torch.Size([2, 3])\n",
        "```\n",
        "위의 코드에서 size() 메서드는 Tensor의 사이즈를 나타내는 torch.Size 클래스를 반환합니다. 이 클래스는 튜플과 유사한 객체로, 각 차원의 크기를 저장합니다.\n",
        "\n",
        "Tensor의 차원과 사이즈는 모두 Tensor의 모양(shape)을 나타내는 데 사용됩니다. 따라서 이들을 정확히 이해하고 사용하는 것은 Tensor의 다양한 연산을 수행하는 데 매우 중요합니다."
      ],
      "metadata": {
        "id": "2OZI88EDHQ6A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 텐서의 차원을 늘리거나 줄이는 방법에 대해서 알려줘"
      ],
      "metadata": {
        "id": "ldpksrEyHSdf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "텐서의 차원을 늘리거나 줄이는 방법은 다음과 같습니다.\n",
        "\n",
        "### unsqueeze() 메서드 사용\n",
        "unsqueeze() 메서드는 Tensor의 차원을 늘리는 데 사용됩니다. 이 메서드는 새로운 차원을 지정하여 Tensor의 모양(shape)을 변경합니다. 예를 들어, 2차원 Tensor를 3차원 Tensor로 변경하려면 다음과 같이 unsqueeze() 메서드를 사용합니다.\n",
        "```\n",
        "import torch\n",
        "\n",
        "# 2차원 Tensor 생성\n",
        "x = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
        "\n",
        "# 차원 늘리기\n",
        "y = x.unsqueeze(0) # 차원의 인덱싱됨을 의미 -> 0번째에 가까울 수록 바깥 차원\n",
        "print(y.shape)  # 출력: torch.Size([1, 2, 3])\n",
        "```\n",
        "\n",
        "### squeeze() 메서드 사용\n",
        "squeeze() 메서드는 Tensor의 차원을 줄이는 데 사용됩니다. 이 메서드는 크기가 1인 차원을 제거하여 Tensor의 모양(shape)을 변경합니다. 예를 들어, 3차원 Tensor를 2차원 Tensor로 변경하려면 다음과 같이 squeeze() 메서드를 사용합니다.\n",
        "\n",
        "```\n",
        "import torch\n",
        "\n",
        "# 3차원 Tensor 생성\n",
        "x = torch.tensor([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])\n",
        "\n",
        "# 차원 줄이기\n",
        "y = x.squeeze(0)\n",
        "print(y.shape)  # 출력: torch.Size([2, 2])\n",
        "```\n",
        "\n",
        "위의 코드에서 squeeze(0)은 첫 번째 차원(0번째 축)의 크기가 1이므로 이 차원을 제거하여 Tensor의 모양(shape)을 (2, 2)으로 변경합니다.\n",
        "\n",
        "### view() 메서드 사용\n",
        "view() 메서드는 Tensor의 모양(shape)을 변경하여 차원을 늘리거나 줄이는 데 사용됩니다. 이 메서드는 Tensor의 원소 수가 변경되지 않도록 유지하면서 모양(shape)을 변경합니다. 예를 들어, 2차원 Tensor를 1차원 Tensor로 변경하려면 다음과 같이 view() 메서드를 사용합니다.\n",
        "```\n",
        "import torch\n",
        "\n",
        "# 2차원 Tensor 생성\n",
        "x = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
        "\n",
        "# 차원 줄이기\n",
        "y = x.view(-1)\n",
        "print(y.shape)  # 출력: torch.Size([6])\n",
        "```\n",
        "위의 코드에서 view(-1)은 Tensor의 원소 수가 동일한 1차원 Tensor로 변경합니다. -1은 Tensor의 모양을 자동으로 계산하도록 지시합니다.\n",
        "\n",
        "view를 통해서 모양을 바꿀 때의 패러미터들은 전체 원소 개수의 약수들의 조합으로 나타낼 수밖에 없음"
      ],
      "metadata": {
        "id": "lVtW4TiqHUGo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* view 메소드의 제약조건이 뭐야?"
      ],
      "metadata": {
        "id": "H0P02MmKHWtn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "view() 메서드는 Tensor의 모양(shape)을 변경하여 차원을 늘리거나 줄이는 데 사용됩니다. 하지만 view() 메서드를 사용할 때 몇 가지 제약 조건이 있습니다.\n",
        "\n",
        "view() 메서드를 사용하여 모양(shape)을 변경할 때, 원래 Tensor와 새로운 Tensor의 원소 수가 동일해야 합니다. 예를 들어, Tensor의 원소 수가 12인 2차원 Tensor를 3차원 Tensor로 변경하려면, 새로운 Tensor의 모양이 (2, 2, 3) 또는 (3, 2, 2)와 같이 원래 Tensor의 원소 수와 동일해야 합니다.\n",
        "\n",
        "view() 메서드를 사용하여 모양(shape)을 변경할 때, 새로운 Tensor의 차원 크기는 0보다 커야 합니다. 즉, view() 메서드로 모든 차원을 삭제할 수는 없습니다.\n",
        "\n",
        "view() 메서드를 사용하여 모양(shape)을 변경할 때, 새로운 Tensor와 원래 Tensor는 메모리 공간을 공유합니다. 따라서 새로운 Tensor를 수정하면 원래 Tensor도 변경됩니다. 이를 방지하려면, clone() 메서드를 사용하여 새로운 메모리 공간을 할당하고 원래 Tensor의 값을 복사하여 새로운 Tensor를 생성할 수 있습니다.\n",
        "\n",
        "일부 Tensor는 view() 메서드를 사용하여 모양(shape)을 변경할 수 없습니다. 이러한 경우, reshape() 메서드를 사용하여 모양(shape)을 변경할 수 있습니다.\n",
        "\n",
        "따라서 view() 메서드를 사용할 때는 위의 제약 조건을 염두에 두고 사용해야 합니다."
      ],
      "metadata": {
        "id": "OTtZ22XRHYe2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 이차원 텐서 (행렬) 연산에서의 합과 곱을 설명해줘 (넘파이에서의 브로드캐스팅 연산과의 차이점도 포함해서)"
      ],
      "metadata": {
        "id": "Ky4Ii-F5HZ__"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "이차원 텐서 (행렬) 연산에서의 합과 곱은 넘파이에서의 브로드캐스팅 연산과는 약간의 차이점이 있습니다.\n",
        "\n",
        "### 합 (Sum)\n",
        "이차원 텐서에서 합을 계산하는 방법은 넘파이에서와 동일합니다. 두 행렬을 더하려면 동일한 위치에 있는 요소를 더하면 됩니다. 예를 들어, 다음과 같이 두 개의 2차원 텐서를 더할 수 있습니다.\n",
        "\n",
        "```\n",
        "import torch\n",
        "\n",
        "# 2차원 Tensor 생성\n",
        "x = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
        "y = torch.tensor([[7, 8, 9], [10, 11, 12]])\n",
        "\n",
        "# 행렬 덧셈\n",
        "z = x + y\n",
        "print(z)\n",
        "# 출력: tensor([[ 8, 10, 12],\n",
        "#              [14, 16, 18]])\n",
        "```\n",
        "\n",
        "### 곱 (Multiplication)\n",
        "이차원 텐서에서 곱을 계산하는 방법은 넘파이에서와 다릅니다. 넘파이에서는 브로드캐스팅 연산을 사용하여 행렬 곱을 계산할 수 있지만, 파이토치에서는 mm() 메서드 또는 matmul() 함수를 사용하여 행렬 곱을 계산합니다. 예를 들어, 다음과 같이 두 개의 2차원 텐서의 행렬 곱을 계산할 수 있습니다.\n",
        "```\n",
        "import torch\n",
        "\n",
        "# 2차원 Tensor 생성\n",
        "x = torch.tensor([[1, 2], [3, 4]])\n",
        "y = torch.tensor([[5, 6], [7, 8]])\n",
        "\n",
        "# 행렬 곱\n",
        "z = torch.mm(x, y)  # 또는 z = torch.matmul(x, y)\n",
        "print(z)\n",
        "# 출력: tensor([[19, 22],\n",
        "#              [43, 50]])\n",
        "```\n",
        "이처럼 파이토치에서는 행렬 곱을 위한 별도의 함수나 메서드를 사용합니다. 따라서 넘파이와는 다른 연산 방식을 사용해야 합니다.\n"
      ],
      "metadata": {
        "id": "ew2HbFs8HbZG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 넘파이에서의 배열곱 예시와 파이토치에서의 배열곱 코드 예시를 각각 하나씩 들어줘"
      ],
      "metadata": {
        "id": "98n8cLgcHdju"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 넘파이에서의 배열곱 예시\n",
        "```\n",
        "import numpy as np\n",
        "\n",
        "# 2차원 배열 생성\n",
        "x = np.array([[1, 2], [3, 4]])\n",
        "y = np.array([[5, 6], [7, 8]])\n",
        "\n",
        "# 배열 곱\n",
        "z = x * y\n",
        "print(z)\n",
        "# 출력: array([[ 5, 12],\n",
        "#              [21, 32]])\n",
        "```\n",
        "위의 예제에서 * 연산자는 넘파이 배열의 요소별 곱셈(element-wise multiplication)을 수행합니다.\n",
        "### 파이토치에서의 배열곱 코드 예시\n",
        "```\n",
        "import torch\n",
        "\n",
        "# 2차원 Tensor 생성\n",
        "x = torch.tensor([[1, 2], [3, 4]])\n",
        "y = torch.tensor([[5, 6], [7, 8]])\n",
        "\n",
        "# 배열 곱\n",
        "z = torch.mul(x, y)\n",
        "print(z)\n",
        "# 출력: tensor([[ 5, 12],\n",
        "#              [21, 32]])\n",
        "```\n",
        "위의 예제에서 mul() 메서드는 파이토치 텐서의 요소별 곱셈을 수행합니다. 따라서 넘파이와 동일한 요소별 곱셈 연산을 수행할 수 있습니다. 만약 파이토치에서 행렬 곱을 수행하려면, mm() 메서드나 matmul() 함수를 사용해야 합니다."
      ],
      "metadata": {
        "id": "sRsXJibsHfZm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "텐서에서의 배열곱 연산에서는 앞의 배열의 열과 뒤의 배열의 행의 길이가 같아야 합니다. 이를 행렬곱 연산의 규칙 중 하나인 \"행렬의 곱셈 조건\"이라고 합니다. 이 규칙은 다음과 같이 표현됩니다.\n",
        "\n",
        "만약, A와 B 두 개의 행렬이 있다면, A의 열의 길이와 B의 행의 길이는 반드시 같아야 합니다. 즉, A의 크기가 (m, n)이고 B의 크기가 (n, p)이라면, A와 B를 곱할 수 있으며 결과 행렬 C의 크기는 (m, p)가 됩니다.\n",
        "\n",
        "이 규칙은 텐서에서의 배열곱 연산에도 동일하게 적용됩니다. 따라서 행렬의 곱셈 조건에 위배되는 텐서들의 배열곱 연산은 불가능합니다."
      ],
      "metadata": {
        "id": "2vUOmiwxHhlP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 텐서에서의 배열곱 연산에서, 행렬의 곱셈 조건을 만족시키는 코드와 만족시키지 못하는 코드에 대해서 예시를 각각 들어줘"
      ],
      "metadata": {
        "id": "3vK3JsA3HoAY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "행렬의 곱셈 조건을 만족시키는 코드와 만족시키지 못하는 코드에 대해서 예시를 각각 들어보겠습니다.\n",
        "\n",
        "### 행렬의 곱셈 조건을 만족시키는 코드\n",
        "```\n",
        "import torch\n",
        "\n",
        "# 2차원 Tensor 생성\n",
        "x = torch.tensor([[1, 2, 3], [4, 5, 6]])  # 크기: (2, 3)\n",
        "y = torch.tensor([[7, 8], [9, 10], [11, 12]])  # 크기: (3, 2)\n",
        "\n",
        "# 행렬 곱\n",
        "z = torch.mm(x, y)  # 크기: (2, 2)\n",
        "print(z)\n",
        "# 출력: tensor([[ 58,  64],\n",
        "#              [139, 154]])\n",
        "```\n",
        "위의 코드에서는 크기가 (2, 3)인 x와 크기가 (3, 2)인 y를 배열곱 연산하여 크기가 (2, 2)인 텐서 z를 생성하고 있습니다. 이 때, x의 열의 길이와 y의 행의 길이가 같으므로 행렬의 곱셈 조건을 만족시키고 있습니다.\n",
        "\n",
        "### 행렬의 곱셈 조건을 만족시키지 못하는 코드\n",
        "```\n",
        "import torch\n",
        "\n",
        "# 2차원 Tensor 생성\n",
        "x = torch.tensor([[1, 2, 3], [4, 5, 6]])  # 크기: (2, 3)\n",
        "y = torch.tensor([[7, 8, 9], [10, 11, 12]])  # 크기: (2, 3)\n",
        "\n",
        "# 배열곱 연산\n",
        "z = torch.mul(x, y)  # 크기: (2, 3)\n",
        "print(z)\n",
        "# 출력: tensor([[ 7, 16, 27],\n",
        "#              [40, 55, 72]])\n",
        "\n",
        "# 행렬 곱 연산\n",
        "z = torch.mm(x, y)  # 오류 발생!\n",
        "```\n",
        "위의 코드에서는 크기가 (2, 3)인 x와 y를 배열곱 연산하여 크기가 (2, 3)인 텐서 z를 생성하고 있습니다. 이 때, x의 열의 길이와 y의 행의 길이가 같지 않으므로 행렬의 곱셈 조건을 만족시키지 못하고 있습니다. 이 경우, 행렬 곱 연산을 수행하면 오류가 발생하게 됩니다."
      ],
      "metadata": {
        "id": "BjY14GBAHqhH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 파이토치에서의 autograd의 개념을, 머신러닝에서의 거리(데이터 개별의 예측값과 정답값 사이의 차이)와 오차(데이터 전체의 예측값과 정답값 사이의 차이) 개념을 바탕으로 경사하강법을 설명하면서 간단하게 정리해줘"
      ],
      "metadata": {
        "id": "UgjFQcvzHsb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "파이토치에서의 Autograd란, 자동 미분(Automatic Differentiation)을 의미하는데, 모델의 학습을 위해 역전파(backpropagation) 알고리즘을 쉽게 구현할 수 있도록 도와줍니다.\n",
        "\n",
        "머신러닝에서의 거리는 데이터 개별의 예측값과 정답값 사이의 차이이고, 이것들의 합이 바로 손실(loss)을 의미합니다. 모델은 이 손실을 최소화하기 위한 파라미터(가중치와 편향)를 찾기 위해 학습됩니다. 이를 위해 경사하강법(Gradient Descent)이라는 최적화 알고리즘이 사용됩니다.\n",
        "\n",
        "경사하강법은 기울기(gradient)를 이용하여 손실을 최소화하는 파라미터를 찾습니다. 이때, 파라미터의 기울기(gradient)를 구하기 위해 역전파 알고리즘이 사용됩니다. 역전파 알고리즘은 Autograd를 통해 자동으로 계산됩니다.\n",
        "\n",
        "Autograd는 파이토치의 Tensor 연산을 추적하여 연산 그래프를 생성합니다. 이 그래프를 이용하여 역전파 알고리즘을 수행하여 파라미터의 기울기(gradient)를 계산합니다. 이후, 경사하강법을 이용하여 손실을 최소화하는 파라미터를 찾습니다.\n",
        "\n",
        "오차는 전체 데이터의 예측값과 정답값 사이의 차이를 의미합니다. 이 오차를 최소화하기 위해 모델이 학습됩니다. Autograd를 이용하여 모델의 파라미터가 갱신될 때마다 손실 함수의 미분값을 자동으로 계산하고, 이를 경사하강법에 이용하여 모델의 파라미터를 갱신합니다.\n",
        "\n",
        "따라서, Autograd는 파이토치에서 머신러닝 모델의 학습을 위한 기반이 되는데, 손실 함수의 미분값을 자동으로 계산하여 모델의 파라미터를 갱신하는 경사하강법에서 중요한 역할을 합니다."
      ],
      "metadata": {
        "id": "ZPVKjiT7HuCA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 경사하강법에 대해서 오차의 최솟값을 찾아가는 내용에 대한 설명으로 구체적으로 해줘"
      ],
      "metadata": {
        "id": "lePr-KyDHvlf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "경사하강법은 오차(loss)를 최소화하는 파라미터(가중치와 편향)를 찾기 위한 최적화 알고리즘입니다. 이를 위해서는 다음과 같은 과정을 거칩니다.\n",
        "\n",
        "1. 초기 파라미터 설정: 모델의 파라미터를 랜덤한 값으로 초기화합니다.\n",
        "1. 손실 함수 계산: 현재의 파라미터로 모델을 실행하여 예측값과 정답값 사이의 차이인 손실(loss)을 계산합니다.\n",
        "1. 기울기 계산: 현재 파라미터에서 손실 함수의 기울기(gradient)를 계산합니다.\n",
        "1. 파라미터 업데이트: 기울기와 학습률(learning rate)을 이용하여 파라미터를 갱신합니다. 이때, 학습률은 파라미터 갱신의 크기를 결정하는 하이퍼파라미터입니다.\n",
        "1. 2~4번 과정을 반복: 새로운 파라미터로 다시 손실 함수를 계산하고, 기울기를 구하고, 파라미터를 업데이트합니다. 이 과정을 일정 횟수나 손실이 더 이상 감소하지 않을 때까지 반복합니다.\n",
        "\n",
        "경사하강법은 손실 함수의 기울기를 이용하여 오차(loss)를 최소화하는 파라미터를 찾아갑니다. 손실 함수의 기울기는 현재 파라미터에서 손실 함수의 값을 가장 많이 감소시키는 방향을 나타냅니다. 따라서, 기울기를 이용하여 파라미터를 업데이트함으로써 오차(loss)가 최소가 되는 파라미터를 찾아갑니다.\n",
        "\n",
        "경사하강법은 손실 함수의 최솟값을 찾아가는 과정에서, 지역 최솟값(local minimum)에 빠지지 않도록 주의해야 합니다. 이를 방지하기 위해 다양한 기법들이 사용되며, 대표적인 예로는 모멘텀(Momentum), 아다그라드(Adagrad), 알엠에스프롭(RMSprop), 아담(Adam) 등이 있습니다. 이러한 기법들은 경사하강법의 성능을 향상시키는데 도움을 줍니다."
      ],
      "metadata": {
        "id": "UEJ-E5CpHxJf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 경사하강법에서 학습률에 대한 개념을 좀 더 자세히 설명해줘"
      ],
      "metadata": {
        "id": "EfojBdZ5HyTX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "경사하강법에서 학습률(learning rate)은 파라미터를 갱신할 때 사용되는 하이퍼파라미터로, 파라미터 갱신의 크기를 결정합니다. 즉, 학습률은 현재 위치에서 기울기가 가리키는 방향으로 얼마나 많이 이동할지를 결정합니다.\n",
        "\n",
        "학습률이 너무 작으면 최적점에 수렴하는 속도가 느리고, 학습률이 너무 크면 발산하여 최적점을 찾을 수 없습니다. 이러한 문제를 방지하기 위해 학습률을 적절하게 설정하는 것이 중요합니다.\n",
        "\n",
        "보통 학습률은 하이퍼파라미터로서, 사람이 직접 설정해주어야 합니다. 하지만, 학습률을 일정하게 고정하는 것보다, 시간에 따라 학습률을 조정하는 방법도 있습니다. 이를 적응적 학습률(Adaptive Learning Rate)이라고 합니다.\n",
        "\n",
        "예를 들어, 아다그라드(Adagrad)와 알엠에스프롭(RMSprop) 등은 학습률을 조정하는 방법 중 하나입니다. 이러한 방법은 학습률을 자동으로 조정하여, 학습을 더욱 효율적으로 수행할 수 있도록 도와줍니다.\n",
        "\n",
        "또한, 최근에는 아담(Adam)이라는 최적화 알고리즘이 많이 사용됩니다. 아담은 모멘텀(Momentum)과 학습률 조정 기능을 모두 갖춘 방법으로, 학습 속도를 빠르게 하면서도 발산을 방지할 수 있습니다."
      ],
      "metadata": {
        "id": "aD7mJ8ymH3YH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 학습률과 과적합의 관계에 대해서 설명해줘"
      ],
      "metadata": {
        "id": "I2rClvFEH4p3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "학습률(learning rate)은 모델의 학습 속도를 결정하는 중요한 하이퍼파라미터입니다. 학습률이 작으면 학습 속도가 느리고, 학습률이 크면 학습 속도가 빨라지지만, 최적점에서 발산할 가능성이 높아집니다. 따라서, 적절한 학습률을 선택하는 것이 중요합니다.\n",
        "\n",
        "과적합(overfitting)은 모델이 훈련 데이터에 과도하게 적합되어, 새로운 데이터에 대한 일반화 성능이 떨어지는 현상입니다. 학습률이 과적합에 영향을 미치는 경우는 다음과 같습니다.\n",
        "\n",
        "학습률이 너무 큰 경우: 학습률이 너무 크면 모델이 최적점을 지나칠 가능성이 높아지므로, 과적합이 발생할 수 있습니다.\n",
        "\n",
        "학습률이 너무 작은 경우: 학습률이 너무 작으면 모델이 최적점에 수렴하는 데 시간이 오래 걸리므로, 과적합이 발생할 수 있습니다.\n",
        "\n",
        "학습률이 적절한 경우: 학습률이 적절하면 모델이 최적점에 수렴하는 속도가 적당하게 조절되므로, 과적합을 방지할 수 있습니다.\n",
        "\n",
        "따라서, 학습률은 과적합을 방지하는데 중요한 역할을 합니다. 적절한 학습률을 선택하여 모델을 학습시키면, 일반화 성능을 높일 수 있습니다. 또한, 과적합을 방지하기 위해 다양한 기법들이 사용됩니다. 대표적인 예로는 정규화(regularization) 기법과 드롭아웃(dropout) 기법이 있습니다. 이러한 기법들을 함께 사용하여 과적합을 방지할 수 있습니다."
      ],
      "metadata": {
        "id": "mrBUIhoGH6M3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 파이토치에서 autograd를 통해서 경사하강법을 구현하는 코드 예시를 들어줘"
      ],
      "metadata": {
        "id": "VQ3ATwuxH7kn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "파이토치에서 autograd를 사용하여 경사하강법을 구현하는 예시 코드를 보여드리겠습니다. 이 코드는 간단한 선형 회귀 모델을 학습하는 코드입니다."
      ],
      "metadata": {
        "id": "TyjiO_x1L1bd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# 데이터셋 정의\n",
        "x_data = torch.tensor([[1.0], [2.0], [3.0]])\n",
        "y_data = torch.tensor([[2.0], [4.0], [6.0]])\n",
        "\n",
        "# 모델 정의\n",
        "class Model(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Model, self).__init__()\n",
        "        self.linear = torch.nn.Linear(1, 1)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        y_pred = self.linear(x)\n",
        "        return y_pred\n",
        "\n",
        "# 모델 초기화\n",
        "model = Model()\n",
        "\n",
        "# 손실 함수 정의\n",
        "criterion = torch.nn.MSELoss()\n",
        "\n",
        "# 옵티마이저 정의\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "# 학습\n",
        "for epoch in range(1000):\n",
        "    # 순전파\n",
        "    y_pred = model(x_data)\n",
        "    \n",
        "    # 손실 계산\n",
        "    loss = criterion(y_pred, y_data)\n",
        "    \n",
        "    # 역전파\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    \n",
        "    # 파라미터 갱신\n",
        "    optimizer.step()\n",
        "    \n",
        "    if epoch % 100 == 0:\n",
        "        print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, 1000, loss.item()))\n",
        "\n",
        "# 테스트\n",
        "x_test = torch.tensor([[4.0]])\n",
        "y_test = model(x_test)\n",
        "print('Prediction:', y_test.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z59ffrn1L4J1",
        "outputId": "5f7d2238-6df6-41a6-a293-6a23d9501e7b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/1000], Loss: 25.2299\n",
            "Epoch [101/1000], Loss: 0.0312\n",
            "Epoch [201/1000], Loss: 0.0193\n",
            "Epoch [301/1000], Loss: 0.0119\n",
            "Epoch [401/1000], Loss: 0.0074\n",
            "Epoch [501/1000], Loss: 0.0045\n",
            "Epoch [601/1000], Loss: 0.0028\n",
            "Epoch [701/1000], Loss: 0.0017\n",
            "Epoch [801/1000], Loss: 0.0011\n",
            "Epoch [901/1000], Loss: 0.0007\n",
            "Prediction: 7.959395408630371\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "이 코드에서는 먼저 데이터셋을 정의하고, 모델을 정의합니다. 모델은 torch.nn.Module 클래스를 상속받아 정의되며, forward 메소드를 통해 모델의 출력값을 계산합니다.\n",
        "\n",
        "그 다음, 손실 함수와 옵티마이저를 정의합니다. 이 예시에서는 평균 제곱 오차(MSE)를 손실 함수로 사용하며, 확률적 경사하강법(SGD)을 옵티마이저로 사용합니다.\n",
        "\n",
        "마지막으로, 학습을 시작합니다. 학습은 1000번의 에폭(epoch) 동안 진행되며, 각 에폭마다 다음과 같은 과정을 거칩니다.\n",
        "\n",
        "순전파: 모델에 입력값을 전달하여 예측값을 계산합니다.\n",
        "손실 계산: 예측값과 정답값의 차이를 계산하여 손실을 구합니다.\n",
        "역전파: 손실 함수의 기울기를 계산하여 모델의 파라미터에 대한 기울기를 구합니다.\n",
        "파라미터 갱신: 옵티마이저를 사용하여 파라미터를 갱신합니다.\n",
        "\n",
        "마지막으로, 테스트를 수행하여 모델의 예측값을 확인합니다. 이 예시에서는 입력값이 4.0일 때의 예측값을 출력합니다.\n",
        "\n",
        "위 코드에서는 autograd를 사용하여 역전파를 수행합니다. loss.backward()를 호출하면, 손실 함수의 기울기가 자동으로 계산되어 모델의 파라미터에 대한 기울기도 자동으로 계산됩니다. 이렇게 자동으로 계산되는 기울기는 .grad 속성을 통해 확인할 수 있습니다.\n",
        "\n",
        "이렇게 autograd를 사용하여 경사하강법을 구현하면, 파라미터 갱신에 대한 구현을 별도로 작성할 필요가 없으므로 코드가 간결해집니다. 또한, 파라미터가 많은 복잡한 모델의 경우에도, autograd를 사용하면 파라미터 갱신을 간단하게 수행할 수 있습니다."
      ],
      "metadata": {
        "id": "pquMgxujMAY9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* nn.Module을 사용하지 않고, require_true와 .grad와 learning_rate를 통해서 수동으로 경사하강법을 구하는 코드 예시를 들어줘"
      ],
      "metadata": {
        "id": "W9Z_K4qsMBU9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "nn.Module을 사용하지 않고, autograd 대신 require_grad와 .grad를 사용하여 경사하강법을 구현하는 코드를 보여드리겠습니다. 이 코드는 간단한 선형 회귀 모델을 학습하는 코드입니다."
      ],
      "metadata": {
        "id": "vKPptlbAMDDF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# 데이터셋 정의\n",
        "x_data = torch.tensor([[1.0], [2.0], [3.0]])\n",
        "y_data = torch.tensor([[2.0], [4.0], [6.0]])\n",
        "\n",
        "# 파라미터 초기화\n",
        "w = torch.tensor([[0.5]], requires_grad=True)\n",
        "b = torch.tensor([[0.5]], requires_grad=True)\n",
        "\n",
        "# 학습률 정의\n",
        "learning_rate = 0.01\n",
        "\n",
        "# 학습\n",
        "for epoch in range(1000):\n",
        "    # 순전파\n",
        "    y_pred = torch.matmul(x_data, w) + b\n",
        "    \n",
        "    # 손실 계산\n",
        "    loss = torch.mean((y_pred - y_data)**2)\n",
        "    \n",
        "    # 역전파\n",
        "    loss.backward()\n",
        "    \n",
        "    # 파라미터 갱신\n",
        "    with torch.no_grad():\n",
        "        w -= learning_rate * w.grad\n",
        "        b -= learning_rate * b.grad\n",
        "        \n",
        "        # 기울기 초기화\n",
        "        w.grad.zero_()\n",
        "        b.grad.zero_()\n",
        "    \n",
        "    if epoch % 100 == 0:\n",
        "        print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, 1000, loss.item()))\n",
        "\n",
        "# 테스트\n",
        "x_test = torch.tensor([[4.0]])\n",
        "y_test = torch.matmul(x_test, w) + b\n",
        "print('Prediction:', y_test.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0nu-BE4iMFtX",
        "outputId": "288af460-f52b-4bcf-cb18-89d731a0ed5d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/1000], Loss: 7.7500\n",
            "Epoch [101/1000], Loss: 0.0837\n",
            "Epoch [201/1000], Loss: 0.0517\n",
            "Epoch [301/1000], Loss: 0.0320\n",
            "Epoch [401/1000], Loss: 0.0198\n",
            "Epoch [501/1000], Loss: 0.0122\n",
            "Epoch [601/1000], Loss: 0.0075\n",
            "Epoch [701/1000], Loss: 0.0047\n",
            "Epoch [801/1000], Loss: 0.0029\n",
            "Epoch [901/1000], Loss: 0.0018\n",
            "Prediction: 7.9334893226623535\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "이 코드에서는 먼저 데이터셋을 정의하고, 파라미터를 초기화합니다. 파라미터 w와 b에 requires_grad=True를 설정하여, autograd를 사용하여 기울기를 계산할 수 있도록 합니다.\n",
        "\n",
        "그 다음, 학습률을 정의하고, 학습을 시작합니다. 학습은 1000번의 에폭(epoch) 동안 진행되며, 각 에폭마다 다음과 같은 과정을 거칩니다.\n",
        "\n",
        "순전파: 모델에 입력값을 전달하여 예측값을 계산합니다.\n",
        "손실 계산: 예측값과 정답값의 차이를 계산하여 손실을 구합니다.\n",
        "역전파: 손실 함수의 기울기를 계산합니다.\n",
        "파라미터 갱신: 수동으로 파라미터를 갱신합니다.\n",
        "\n",
        "`with torch.no_grad()를 사용하여 파라미터 갱신을 수동으로 수행합니다. w와 b의 .grad 속성을 사용하여 기울기를 확인하고, .zero_()를 사용하여 기울기를 초기화합니다.`\n",
        "\n",
        "마지막으로, 테스트를 수행하여 모델의 예측값을 확인합니다. 이 예시에서는 입력값이 4.0일 때의 예측값을 출력합니다.\n",
        "\n",
        "위 코드에서는 autograd를 사용하지 않고, .requires_grad와 .grad를 사용하여 경사하강법을 구현합니다. 이 방식은 autograd를 사용하는 방식보다는 조금 더 복잡하고, 계산 과정이 수동으로 이루어져야 하기 때문에, 일반적으로 사용되는 방식은 아닙니다. 하지만, autograd의 개념을 이해하는 데에 도움이 될 수 있습니다."
      ],
      "metadata": {
        "id": "sMar4-5-MHu2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ANN"
      ],
      "metadata": {
        "id": "O7t0lDdNMEff"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 퍼셉트론과 입력층, 은닉층, 출력층 개념을 바탕으로 ANN에 대해서 설명해줘"
      ],
      "metadata": {
        "id": "CKBUV7UaNitg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "인공 신경망(ANN, Artificial Neural Network)은 생물학적 뉴런의 동작 원리를 모방하여 만들어진 모델로, 입력층(input layer), 은닉층(hidden layer), 출력층(output layer)으로 구성됩니다.\n",
        "\n",
        "먼저, 입력층은 입력 데이터를 받아들이는 역할을 합니다. 입력 데이터는 하나의 벡터로 표현되며, 각 원소는 하나의 특성(feature)을 나타냅니다.\n",
        "\n",
        "다음으로, 입력층과 출력층 사이에는 하나 이상의 은닉층이 존재합니다. 은닉층은 입력 데이터에서 중간 계산 결과를 만드는 역할을 합니다. 이 중간 계산 결과는 다시 출력층으로 전달되어 최종 출력값을 만들게 됩니다. 은닉층은 여러 개의 뉴런(neuron)으로 이루어져 있으며, 각 뉴런은 이전 계층의 출력값을 입력으로 받아 가중치(weight)와 편향(bias)을 고려하여 출력값을 계산합니다.\n",
        "\n",
        "마지막으로, 출력층은 최종 출력값을 만드는 역할을 합니다. 출력층은 일반적으로 분류 문제인 경우에는 클래스에 대한 확률 값을 출력하고, 회귀 문제인 경우에는 예측값을 출력합니다.\n",
        "\n",
        "퍼셉트론(perceptron)은 가장 단순한 형태의 인공 신경망 모델로, 입력층과 출력층 사이에 하나의 뉴런만 존재합니다. 퍼셉트론은 이진 분류(binary classification) 문제를 해결하기 위해 개발되었으며, 하나의 입력 벡터가 주어졌을 때, 이 입력 벡터를 가중치와 편향으로 계산한 결과값이 임계값(threshold)보다 크면 1을 출력하고, 작으면 0을 출력하는 형태로 동작합니다.\n",
        "\n",
        "하지만, 퍼셉트론은 XOR 문제와 같은 선형 분리가 불가능한 문제를 해결할 수 없다는 한계점이 있습니다. 이를 해결하기 위해, 은닉층을 추가한 다층 퍼셉트론(multi-layer perceptron, MLP)이 등장하게 되었습니다. MLP는 여러 개의 은닉층을 추가하여 비선형성을 추가한 모델로, 더 복잡한 문제를 해결할 수 있습니다."
      ],
      "metadata": {
        "id": "9bGIb93cNmhu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MLP를 통해서 XOR 문제를 해결하는 파이토치 모델을 짜줘"
      ],
      "metadata": {
        "id": "14e2gob4NoBG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "다층 퍼셉트론(MLP)을 사용하여 XOR 문제를 해결하는 파이토치 모델을 구현해보겠습니다. 이 예시에서는 2개의 은닉층을 사용하며, 각 은닉층의 뉴런 수는 2개입니다."
      ],
      "metadata": {
        "id": "IMn9E9XSNpm2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# XOR 문제 데이터셋\n",
        "x_data = torch.tensor([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=torch.float32)\n",
        "y_data = torch.tensor([[0], [1], [1], [0]], dtype=torch.float32)\n",
        "\n",
        "# 모델 정의\n",
        "class XORModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(XORModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(2, 2)\n",
        "        self.fc2 = nn.Linear(2, 2)\n",
        "        self.fc3 = nn.Linear(2, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = torch.sigmoid(self.fc3(x))\n",
        "        return x\n",
        "\n",
        "# 모델 생성\n",
        "model = XORModel()\n",
        "\n",
        "# 손실 함수 정의\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "# 옵티마이저 정의\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
        "\n",
        "# 학습\n",
        "for epoch in range(10000):\n",
        "    # 순전파\n",
        "    y_pred = model(x_data)\n",
        "\n",
        "    # 손실 계산\n",
        "    loss = criterion(y_pred, y_data)\n",
        "\n",
        "    # 기울기 계산 및 역전파\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "\n",
        "    # 파라미터 갱신\n",
        "    optimizer.step()\n",
        "\n",
        "    if epoch % 1000 == 0:\n",
        "        print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, 10000, loss.item()))\n",
        "\n",
        "# 테스트\n",
        "with torch.no_grad():\n",
        "    test_input = torch.tensor([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=torch.float32)\n",
        "    test_output = model(test_input)\n",
        "    print(test_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j1yNUaZqNv5W",
        "outputId": "dc893311-d14d-4e6d-8eb0-54d668ada99e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10000], Loss: 0.7475\n",
            "Epoch [1001/10000], Loss: 0.3502\n",
            "Epoch [2001/10000], Loss: 0.3476\n",
            "Epoch [3001/10000], Loss: 0.3471\n",
            "Epoch [4001/10000], Loss: 0.3469\n",
            "Epoch [5001/10000], Loss: 0.3468\n",
            "Epoch [6001/10000], Loss: 0.3468\n",
            "Epoch [7001/10000], Loss: 0.3467\n",
            "Epoch [8001/10000], Loss: 0.3467\n",
            "Epoch [9001/10000], Loss: 0.3467\n",
            "tensor([[3.5778e-04],\n",
            "        [4.9988e-01],\n",
            "        [9.9988e-01],\n",
            "        [4.9988e-01]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "위 코드에서는 nn.Module을 상속받아 XORModel 클래스를 정의합니다. XORModel 클래스는 __init__()에서 2개의 은닉층과 1개의 출력층을 정의하며, forward()에서는 입력값에 대한 순전파 연산을 수행합니다. 은닉층의 활성화 함수는 ReLU 함수를 사용하고, 출력층의 활성화 함수는 시그모이드 함수를 사용합니다.\n",
        "\n",
        "학습에 사용되는 손실 함수는 이진 교차 엔트로피 오차(BCELoss)를 사용하며, 옵티마이저는 확률적 경사 하강법(SGD)을 사용합니다.\n",
        "\n",
        "학습은 10000번의 에폭(epoch) 동안 진행되며, 각 에폭마다 다음과 같은 과정을 거"
      ],
      "metadata": {
        "id": "yvN1uQwKNq_f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## xor문제를 왜 단순한 퍼셉트론이 해결할 수 없지?"
      ],
      "metadata": {
        "id": "1FWNX_eaNzrG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "XOR 문제는 단순한 퍼셉트론으로는 해결할 수 없는 문제입니다. 이유는 XOR 연산이 선형 분리가 불가능한 비선형적인 문제이기 때문입니다.\n",
        "\n",
        "단순한 퍼셉트론은 입력값들을 가중치와 편향을 이용하여 선형적으로 분리하는 선형 분류기의 한 종류입니다. 즉, 입력값들의 선형 결합으로 클래스를 분류합니다. 그러나 XOR 문제는 입력값들을 선형 결합으로 분리할 수 없기 때문에, 단순한 퍼셉트론으로는 해결할 수 없습니다.\n",
        "\n",
        "이를 해결하기 위해서는 더 복잡한 비선형 모델이 필요하며, 다층 퍼셉트론(multi-layer perceptron, MLP)을 사용하여 XOR 문제를 해결할 수 있습니다. MLP는 여러 개의 은닉층을 추가하여 비선형성을 추가한 모델로, 더 복잡한 문제를 해결할 수 있습니다.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "27a0JZ1IN1aH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 그러면 은닉층 하나는 하나의 특징을 잡아낸다고 보면 되겠네?\n"
      ],
      "metadata": {
        "id": "H88WL1RvN5vm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "아니요, 은닉층 하나가 하나의 특징(feature)을 잡아낸다고 보는 것은 정확하지 않습니다. 은닉층은 입력 데이터에서 중간 계산 결과를 만드는 역할을 합니다. 이 중간 계산 결과는 다시 출력층으로 전달되어 최종 출력값을 만들게 됩니다.\n",
        "\n",
        "따라서, 은닉층 하나는 하나 이상의 특징을 잡아낼 수 있습니다. 은닉층의 뉴런 수가 많아질수록 더 많은 특징을 잡아낼 수 있습니다. 그리고, 각 뉴런은 이전 계층의 출력값을 입력으로 받아 가중치와 편향을 고려하여 출력값을 계산합니다. 이러한 계산 과정에서 비선형성이 추가됩니다. 이러한 비선형성은 다층 퍼셉트론이 선형 분리가 불가능한 비선형적인 문제를 해결할 수 있는 이유 중 하나입니다."
      ],
      "metadata": {
        "id": "tPzrYVkvN7Am"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 은닉층이 늘어날 수록 비선형적 문제를 해결할 수 있겠네?"
      ],
      "metadata": {
        "id": "JYEp5_n0N8rv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "네, 은닉층이 늘어날수록 비선형적 문제를 해결할 수 있는 가능성이 높아집니다. 은닉층이 추가될수록 모델이 복잡해지며, 더 복잡한 결정 경계(decision boundary)를 만들어 내기 때문입니다.\n",
        "\n",
        "하지만, 은닉층을 너무 많이 추가할 경우에는 과적합(overfitting)이 발생할 수 있습니다. 과적합은 모델이 학습 데이터에 과도하게 적합화되어 새로운 데이터에 대한 성능이 저하되는 현상입니다. 따라서, 은닉층을 추가할 때에는 학습 데이터에만 과도하게 적합화되지 않도록 적절한 조절이 필요합니다."
      ],
      "metadata": {
        "id": "OMa__lsGN9Y2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BCELoss 손실함수에 대해서 좀 더 자세히 설명해줘\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ILYoPU4VN-td"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "BCELoss는 이진 분류(Binary Classification) 문제에서 사용되는 손실 함수입니다. BCE는 Binary Cross Entropy의 약자이며, 이진 분류 문제에서 예측값과 실제값 사이의 차이를 계산하여 손실값을 구하는데 사용됩니다.\n",
        "\n",
        "BCELoss는 예측값과 실제값이 각각 0 또는 1인 경우에는 무한대로 발산하므로, 학습이 불가능한 상태가 됩니다. 따라서, BCELoss에서는 일정한 범위 안에서 손실값을 유지하기 위해 로그 함수를 적용한 뒤, 손실값을 계산합니다. 이를 로그 손실(log loss)이라고도 부릅니다.\n",
        "\n",
        "BCELoss는 다음과 같은 수식으로 정의됩니다.\n",
        "\n",
        "여기서 y는 실제값, ŷ은 예측값입니다. BCELoss는 이진 분류에서만 사용되며, 다중 분류(multi-class classification) 문제에서는 Cross Entropy Loss 등 다른 손실 함수를 사용합니다."
      ],
      "metadata": {
        "id": "u2cUp34BODmG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 다중 분류(multi-class classification) 문제에서는 Cross Entropy Loss 손실 함수에 대해서 설명해줘"
      ],
      "metadata": {
        "id": "ML-V5CIkOE0e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cross Entropy Loss는 분류 문제에서 사용되는 손실 함수 중 하나입니다. 다중 분류(multi-class classification) 문제에서 사용됩니다. Cross Entropy Loss는 모델이 예측한 확률 분포와 실제 레이블에 대한 확률 분포의 차이를 계산합니다.\n",
        "\n",
        "Cross Entropy Loss는 다음과 같은 수식으로 정의됩니다.\n",
        "\n",
        "여기서 y는 실제값의 확률 분포, ŷ은 모델이 예측한 확률 분포입니다. Cross Entropy Loss는 y와 ŷ의 차이를 계산하며, 이 차이가 작을수록 손실값은 작아집니다.\n",
        "\n",
        "Cross Entropy Loss는 다중 분류 문제에서 일반적으로 사용되는 손실 함수 중 하나입니다. Cross Entropy Loss의 장점은 손실 함수가 미분 가능하며, 경사 하강법을 통해 최적화할 수 있다는 점입니다. 따라서 Cross Entropy Loss는 딥러닝 모델의 학습에서 자주 사용됩니다."
      ],
      "metadata": {
        "id": "5klecwpwOGhV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DNN"
      ],
      "metadata": {
        "id": "2NLeg213P9IQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DNN에 대한 개념을 심층 심경망을 바탕으로 설명해줘"
      ],
      "metadata": {
        "id": "IEyjaFzFP-Wu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "DNN은 Deep Neural Network의 약자로, 층(layer)을 깊게 쌓은 신경망을 의미합니다. DNN은 기존의 얕은 신경망보다 깊게 쌓은 층을 사용하여 더 복잡한 모델을 만들 수 있습니다.\n",
        "\n",
        "심층 신경망(Deep Neural Network)은 일반적으로 입력층, 은닉층(hidden layer), 출력층으로 구성됩니다. 입력층은 데이터를 받아들이는 역할을 하고, 출력층은 최종 결과값을 출력하는 역할을 합니다. 은닉층은 입력층과 출력층 사이에 위치하며, 입력값을 처리하여 출력값을 만드는 역할을 합니다.\n",
        "\n",
        "DNN에서는 은닉층을 여러 개 쌓아서 복잡한 비선형성을 모델링할 수 있습니다. 각 은닉층에서는 다양한 활성화 함수(activation function)를 사용하여 비선형성을 추가합니다. 대표적인 활성화 함수로는 ReLU(Rectified Linear Unit), sigmoid, tanh 등이 있습니다.\n",
        "\n",
        "DNN은 딥러닝 분야에서 많이 사용되며, 이미지 분류, 음성 인식, 자연어 처리 등 다양한 분야에서 우수한 성능을 보입니다. 하지만, DNN은 학습하기 어렵고, 초기값 설정과 하이퍼파라미터 조정 등이 중요합니다. 또한, 학습 데이터가 많을수록 DNN의 성능이 향상됩니다."
      ],
      "metadata": {
        "id": "PsAGMsj5P_5n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 활성화 함수(activation function)가 뭐지?"
      ],
      "metadata": {
        "id": "vQtiUPSoQB4G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "활성화 함수(activation function)은 인공신경망에서 각 뉴런의 출력값을 결정하는 함수입니다. 입력값을 받아서 출력값을 계산하는 함수로서, 인공신경망에서 가장 중요한 구성요소 중 하나입니다.\n",
        "\n",
        "활성화 함수는 비선형성을 추가하여 뉴런의 출력값을 비선형적으로 만들어줍니다. 인공신경망에서는 비선형성이 중요한 이유는 입력값과 출력값 사이의 관계가 비선형적인 경우가 많기 때문입니다. 따라서, 선형 함수만 사용하면 인공신경망이 단순한 선형 회귀 모델과 다르지 않게 됩니다.\n",
        "\n",
        "대표적인 활성화 함수로는 sigmoid, tanh, ReLU(Rectified Linear Unit) 등이 있습니다. sigmoid 함수는 입력값을 0과 1 사이의 값으로 변환하여, 뉴런의 출력값을 확률값으로 해석할 수 있습니다. tanh 함수는 sigmoid 함수와 비슷하지만, 출력값의 범위가 -1과 1 사이입니다. ReLU 함수는 입력값이 양수일 때는 입력값을 그대로 출력하고, 음수일 때는 0을 출력하는 함수입니다.\n",
        "\n",
        "활성화 함수는 인공신경망의 학습에 매우 중요한 역할을 합니다. 적절한 활성화 함수를 선택하고, 이를 적용하는 것이 인공신경망의 성능 향상에 매우 중요합니다."
      ],
      "metadata": {
        "id": "pP6Aat9iQDdu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 심층 심경망에서 sigmoid 대신 relu를 주로 쓰는 이유가 뭐지?"
      ],
      "metadata": {
        "id": "KAWvvM6kQF7H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "심층 신경망에서 ReLU(Rectified Linear Unit)를 주로 사용하는 이유는 다음과 같습니다.\n",
        "\n",
        "1. 빠른 연산 속도: ReLU는 sigmoid 함수보다 더 빠르게 계산됩니다. sigmoid 함수는 지수 함수 계산을 수행하기 때문에 연산이 느릴 수 있습니다. 반면에 ReLU 함수는 입력값이 양수일 때는 입력값을 그대로 출력하고, 음수일 때는 0을 출력하기 때문에, 연산이 간단하고 빠릅니다.\n",
        "1. 비선형성: ReLU는 입력값이 0보다 작을 때 0을 출력하기 때문에 비선형성을 가집니다. 비선형성을 가지는 활성화 함수를 사용하면, 인공신경망이 복잡한 비선형성을 모델링할 수 있습니다.\n",
        "1. 기울기 소실 문제 해결: sigmoid 함수는 입력값이 큰 경우, 기울기가 매우 작아져 학습이 어려워지는 기울기 소실 문제가 발생할 수 있습니다. 하지만 ReLU는 입력값이 양수일 때는 항상 기울기가 1이기 때문에, 기울기 소실 문제를 해결할 수 있습니다.\n",
        "\n",
        "따라서, ReLU는 연산 속도가 빠르고, 비선형성을 가지며, 기울기 소실 문제를 해결할 수 있기 때문에, 심층 신경망에서 주로 사용됩니다."
      ],
      "metadata": {
        "id": "K2BMEhrMQHGG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DNN에서의 기울기 소실 문제가 뭐야?"
      ],
      "metadata": {
        "id": "wis153jqQIkG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "DNN(Deep Neural Network)에서의 기울기 소실 문제는 역전파(backpropagation) 과정에서 출력층에서 입력층으로 거슬러 올라갈 때, 기울기 값이 점차 작아지면서 학습이 제대로 이루어지지 않는 문제를 의미합니다.\n",
        "\n",
        "기울기 소실 문제는 일반적으로 sigmoid 함수나 tanh 함수 등의 활성화 함수를 사용할 때 발생합니다. 이 함수들은 출력값을 0과 1 또는 -1과 1 사이의 값으로 제한하는 특성을 가지고 있습니다. 따라서, 출력값이 극단에 가까워지면 기울기가 매우 작아져 역전파 과정에서 이전 층으로 전달되는 기울기 값이 매우 작아져버리기 때문에, 학습이 제대로 이루어지지 않습니다.\n",
        "\n",
        "기울기 소실 문제를 해결하는 방법으로는, ReLU(Rectified Linear Unit)와 같은 활성화 함수를 사용하는 것이 있습니다. ReLU는 입력값이 양수일 때는 입력값을 그대로 출력하고, 음수일 때는 0을 출력하기 때문에, 기울기 소실 문제를 해결할 수 있습니다. 또한, 초기값 설정 등의 하이퍼파라미터를 조정하여 기울기 소실 문제를 해결할 수도 있습니다."
      ],
      "metadata": {
        "id": "LXbcSG7DQJuQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DNN을 활용해서 Fashion MNEST 데이터를 10개의 패션 아이템으로 분류하는 이미지 분류 모델의 예시 코드를 구현해줘, 은닉층은 3개 정도만 사용해서"
      ],
      "metadata": {
        "id": "0DCKPV6KQLne"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "아래는 PyTorch를 사용하여 Fashion MNIST 데이터셋을 이용하여 DNN 모델을 구현하는 예시 코드입니다. 이 모델은 10개의 클래스로 분류하는 이미지 분류 모델입니다. 은닉층은 3개를 사용하며, ReLU 활성화 함수를 사용합니다.\n",
        "\n"
      ],
      "metadata": {
        "id": "B6srMNbnQNNh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision.datasets as dsets\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# 데이터셋 불러오기\n",
        "train_dataset = dsets.FashionMNIST(root='./data', train=True, transform=transforms.ToTensor(), download=True)\n",
        "test_dataset = dsets.FashionMNIST(root='./data', train=False, transform=transforms.ToTensor(), download=True)\n",
        "\n",
        "# 데이터 로더 설정\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=100, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=100, shuffle=False)\n",
        "\n",
        "# DNN 모델 클래스 정의\n",
        "class DNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(28*28, 256)\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.fc3 = nn.Linear(128, 64)\n",
        "        self.fc4 = nn.Linear(64, 10)\n",
        "        self.relu = nn.ReLU()\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28*28)\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.relu(self.fc2(x))\n",
        "        x = self.relu(self.fc3(x))\n",
        "        x = self.fc4(x)\n",
        "        return x\n",
        "\n",
        "# 모델 인스턴스 생성\n",
        "model = DNN()\n",
        "\n",
        "# 손실 함수 및 최적화 함수 정의\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# 학습 수행\n",
        "for epoch in range(10):\n",
        "    running_loss = 0.0\n",
        "    for i, (inputs, labels) in enumerate(train_loader):\n",
        "        # 입력값과 레이블 초기화\n",
        "        inputs, labels = inputs.cuda(), labels.cuda()\n",
        "        \n",
        "        # 모델 학습\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        # 손실 함수 값 출력\n",
        "        running_loss += loss.item()\n",
        "        if i % 100 == 99:\n",
        "            print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 100))\n",
        "            running_loss = 0.0\n",
        "\n",
        "# 모델 테스트 수행\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "        # 입력값과 레이블 초기화\n",
        "        inputs, labels = inputs.cuda(), labels.cuda()\n",
        "        \n",
        "        # 모델 테스트\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print('Accuracy of the network on the 10000 test images: %d %%' % (100 * correct / total))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 403
        },
        "id": "VLvAQY4mQOf2",
        "outputId": "91714f34-f235-4236-bf7e-e3379547d65c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-cfd0c54506b8>\u001b[0m in \u001b[0;36m<cell line: 41>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;31m# 모델 학습\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-cfd0c54506b8>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_addmm)"
          ]
        }
      ]
    }
  ]
}