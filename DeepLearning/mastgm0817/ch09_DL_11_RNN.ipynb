{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mastgm0817/DeepLearning/blob/main/mastgm0817/ch09_DL_11_RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 영화 리뷰 감정 분석\n",
        "* RNN은 동영상, 자연어, 주가 등 동적인 데이터를 이용할 때 성능이 극대화\n",
        "* 자연어 : 일상적으로 사용하는 말을 프로그래밍 언어와 구분하여 부르는 말"
      ],
      "metadata": {
        "id": "uUNeCTOxh9xo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* IMDB 데이터셋\n",
        "    * 텍스트 형태의 데이터셋\n",
        "    * 50,000건의 영화 리뷰로 이루어져 있음\n",
        "    * 각 리뷰는 다수의 영어 문장들로 이루어져 있으며, 평점이 7점 이상의 긍정적인 영화 리뷰는 2로, 평점이 4점 이하인 부정적인 영화 리뷰는 1로 레이블링"
      ],
      "metadata": {
        "id": "PuJJHJMxiPVH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 영화 리뷰 텍스트를 RNN에 입력시켜 영화평의 전체 내용을 압축하고, 이렇게 압축된 리뷰 긍정적인지 부정적인지 판단해주는 간단한 분류 모델을 생성\n",
        "* 한글 데이터도 적용"
      ],
      "metadata": {
        "id": "dFncRVweihga"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 워드 임베딩 word embedding: 언어의 최소 단위 -> 토큰화 => 벡터화 (라벨 인코딩)"
      ],
      "metadata": {
        "id": "N9Bpl4PVi0Tz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 자연어 전처리"
      ],
      "metadata": {
        "id": "fLDMNqjHjPVR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lS0DyICgh4t7",
        "outputId": "8aa6ff48-a938-4c85-e8a2-4e02dfe2f3cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: torch\n",
            "Version: 2.0.0+cu118\n",
            "Summary: Tensors and Dynamic neural networks in Python with strong GPU acceleration\n",
            "Home-page: https://pytorch.org/\n",
            "Author: PyTorch Team\n",
            "Author-email: packages@pytorch.org\n",
            "License: BSD-3\n",
            "Location: /usr/local/lib/python3.9/dist-packages\n",
            "Requires: filelock, jinja2, networkx, sympy, triton, typing-extensions\n",
            "Required-by: fastai, torchaudio, torchdata, torchtext, torchvision, triton\n",
            "---\n",
            "Name: torchtext\n",
            "Version: 0.15.1\n",
            "Summary: Text utilities and datasets for PyTorch\n",
            "Home-page: https://github.com/pytorch/text\n",
            "Author: PyTorch core devs and James Bradbury\n",
            "Author-email: jekbradbury@gmail.com\n",
            "License: BSD\n",
            "Location: /usr/local/lib/python3.9/dist-packages\n",
            "Requires: numpy, requests, torch, torchdata, tqdm\n",
            "Required-by: \n"
          ]
        }
      ],
      "source": [
        "!pip show torch torchtext\n",
        "# torch 1.9.1 / torchtext 0.10.1 (0.13.0 이전 사용)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 특수한 버전으로 만들어주겠다 -> 랜타임 다시 시작\n",
        "!pip install torchtext==0.10.1 --user -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sjL4bssDjxVl",
        "outputId": "df5dda92-9d78-41ea-e145-9298b328eab6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m831.4/831.4 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33m  WARNING: The scripts convert-caffe2-to-onnx and convert-onnx-to-caffe2 are installed in '/root/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.15.1+cu118 requires torch==2.0.0, but you have torch 1.9.1 which is incompatible.\n",
            "torchdata 0.6.0 requires torch==2.0.0, but you have torch 1.9.1 which is incompatible.\n",
            "torchaudio 2.0.1+cu118 requires torch==2.0.0, but you have torch 1.9.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 필수 라이브러리 임포트\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchtext.legacy import data, datasets"
      ],
      "metadata": {
        "id": "R9IOmlG7kGyO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 하이퍼 패러미터 정의\n",
        "BATCH_SIZE = 64\n",
        "lr = 0.001\n",
        "EPOCHS = 10\n",
        "USE_CUDA = torch.cuda.is_available()\n",
        "DEVICE = torch.device(\"cuda\" if USE_CUDA else \"cpu\")\n",
        "print(f'running in {DEVICE}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oGMPhU6yk2PN",
        "outputId": "15d6dd77-9181-47a6-a92f-681f42c5220b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "running in cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 로딩하기\n",
        "TEXT = data.Field(sequential=True, batch_first=True, lower=True) # 데이터를 받아줄 빈 객체\n",
        "# sequential : 순서가 있는 데이터인가? / batch_first : 배치 사이즈 차원을 맨 앞으로 두겠다 / lower : 모두 소문자로 전처리\n",
        "LABEL = data.Field(sequential=False, batch_first=True) # 데이터를 받아줄 빈 객체\n",
        "# 0, 1 긍부정 여부 데이터"
      ],
      "metadata": {
        "id": "_9d_GMs9lkES"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 훈련셋, 테스트셋으로 분리\n",
        "trainset, testset = datasets.IMDB.splits(TEXT, LABEL)"
      ],
      "metadata": {
        "id": "JGAiSU5pmiNR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"훈련셋 : {len(trainset)}\")\n",
        "print(f\"시험셋 : {len(testset)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hL-jr7qImzcI",
        "outputId": "1d023296-cee0-4343-833b-49c6ca1db558"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "훈련셋 : 25000\n",
            "시험셋 : 25000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainset[0].text[:10] # 자동으로 전처리가 되어 토큰화된 데이터 묶음"
      ],
      "metadata": {
        "id": "MBsTyS7-nHKa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 단어 사전\n",
        "TEXT.build_vocab(trainset, min_freq=5) # 5번 이상 등장한 단어로 단어 사전 만들기\n",
        "# 학습데이터에서 5번 미만 등장한 데이터는 unk(unknown)으로 대체\n",
        "LABEL.build_vocab(trainset)"
      ],
      "metadata": {
        "id": "HgpjHenFna5k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TEXT.vocab.freqs.most_common(100) # 단어의 등장 빈도"
      ],
      "metadata": {
        "id": "7dZz_lzln2by"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LABEL.vocab.freqs"
      ],
      "metadata": {
        "id": "YM4KxscLoN68"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 학습용 데이터를 학습셋 80% 검증셋 20%로 나누기\n",
        "trainset, valset = trainset.split(split_ratio=0.8)"
      ],
      "metadata": {
        "id": "2xLLRNJMtl0x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 배치 로딩을 위한 데이터 로더\n",
        "# 텍스트 형태의 데이터도 모든 학습 데이터를 한 번에 처리 X\n",
        "# batch(배치) 단위로 쪼개서 학습을 진행 -> 반복할 때마다 배치를 생성해주는 반복자(iterator)\n",
        "train_iter, val_iter, test_iter = data.BucketIterator.splits(\n",
        "    (trainset, valset, testset),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True, repeat=False\n",
        ")\n",
        "# 반복자 -> enumerate() 함수에 넣어서 반복해주면(루프) 배치 단위의 데이터셋만 뽑아올 수 있음"
      ],
      "metadata": {
        "id": "ArqdfH65t58I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 사전 속 단어의 개수, 레이블의 수\n",
        "vocab_size = len(TEXT.vocab)\n",
        "n_classes = 2"
      ],
      "metadata": {
        "id": "8nAJ4vKIutKg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"[학습셋] {len(trainset)} / [검증셋] {len(valset)} / 테스트셋 {len(testset)} / [단어수] {vocab_size} / [클래스(라벨)] {n_classes}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gtpsx2xmu0HX",
        "outputId": "910f2717-1afc-4026-8c70-1d5ab39d4f2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[학습셋] 20000 / [검증셋] 5000 / 테스트셋 25000 / [단어수] 46159 / [클래스(라벨)] 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RNN 모델 구현"
      ],
      "metadata": {
        "id": "VzZwbGL9vQDh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BasicGRU(nn.Module):\n",
        "    def __init__(self,\n",
        "                 n_layers, # 층 -> 은닉 벡터들의 층의 갯수 : 엄청 복잡한 모델이 아니라면, n_layers는 2 이하 통상 정의\n",
        "                 hidden_dim, # 층 내부의 너비\n",
        "                 n_vocab, # 단어 임베딩 관련\n",
        "                 embed_dim, # 단어 임베딩 관련\n",
        "                 n_classes, # 최종적으로 분류할 텍스트의 가짓수 (neg, pos)\n",
        "                 dropout_p=0.2\n",
        "                 ):\n",
        "        super(BasicGRU, self).__init__()\n",
        "        self.n_layers = n_layers\n",
        "        # 단어 임베딩 -> 단어들을 벡터화(라벨링)\n",
        "        self.embed = nn.Embedding(n_vocab, embed_dim)\n",
        "        # n_vocab : 전체 데이터셋의 모든 단어를 사전 형태로 나타냈을 때, 그 사전에 등재된 단어 수\n",
        "        # embed_dim : 임베딩 -> 단어 텐서가 가지는 차원값\n",
        "        # RNN을 통해 생성되는 은닉 벡터의 차원값과 드롭아웃을 정의\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "\n",
        "        # 단어 임베딩을 거친 텐서가 GRU로 입력\n",
        "        self.gru = nn.GRU(embed_dim, self.hidden_dim,\n",
        "                          num_layers=self.n_layers, batch_first=True)\n",
        "        # 시계열(순서) 데이터 -> 하나의 텐서 압축\n",
        "        self.out = nn.Linear(hidden_dim, n_classes)\n",
        "    \n",
        "    def forward(self, x): # 문장들 \n",
        "        x = self.embed(x) # 단어들이 일괄적으로 숫자값을 가지는 텐서로 변환\n",
        "        h_0 = self._init_state(batch_size=x.size(0)) # 젓번째 은닉 벡터 초기값 (H0)\n",
        "        x, _ = self.gru(x, h_0) # 입력 x를 첫번째 은닉 벡터 h_0과 함께 gru에 입력하면\n",
        "        # -> 은닉 벡터들이 시계열 배열 형태로 반환 -> (batch_size, 입력 x의 길이, hidden_dim) 3차원 텐서\n",
        "        # gru를 통해 나온 텐서 x -> [:, -1, :]\n",
        "        # -> 배치 내 모든 시계열 은닉 벡터들의 마지막 토큰들을 내포한 (batch_size, 1, hidden_dim)의 텐서를 추출\n",
        "        # => 텐서 내에 가장 최신 은닉값\n",
        "        h_t = x[:, -1, :] # 모든 데이터를 반영한 핵심값 (압축된 은닉 벡터)\n",
        "        # 특정한 영화평, 영화리뷰에 있는 모든 단어(토큰)들을 압축한 값\n",
        "        self.dropout(h_t)\n",
        "        logit = self.out(h_t)\n",
        "        return logit\n",
        "\n",
        "    def _init_state(self, batch_size=1):\n",
        "        # parameters() : 해당 신경망 모듈(nn.Module)의 가중치 정보를 iterator 형태로 반환\n",
        "        weight = next(self.parameters()).data # 실제 모델의 가중치\n",
        "        return weight.new(self.n_layers, batch_size, self.hidden_dim).zero_()\n",
        "        # 기존에 존재하던 가중치와 연결한 뒤 순전파에 사용되는 모양으로 바꾸고, 0으로 초기화"
      ],
      "metadata": {
        "id": "zyzVBCiMvSPI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 학습함수\n",
        "def train(model, optimizer, train_iter):\n",
        "    model.train()\n",
        "    for b, batch in enumerate(train_iter):\n",
        "        # b : 각 행의 인덱스, batch : 미니 배치 (학습을 위해 구분한 배치 사이즈 데이터)\n",
        "        x, y = batch.text.to(DEVICE), batch.label.to(DEVICE)\n",
        "        # y = pos, neg -> vocab -> 1, 2 -> 0, 1\n",
        "        y.data.sub_(1) # y값에 일괄적으로 1를 빼줘서, 1과 2를 0과 1로 변환\n",
        "        optimizer.zero_grad() # 최적화함수 -> 기울기 계산 리셋\n",
        "        # x를 모델에 입력해서 예측값인 logit 계산\n",
        "        logit = model(x) # logit = 0과 1 사이의 확률\n",
        "        # 손실 함수 -> 손실\n",
        "        loss = F.cross_entropy(logit, y) # 예측값과 정답값(라벨) 비교 -> 손실함수\n",
        "        loss.backward() # 역전파\n",
        "        optimizer.step() # 가중치 갱신"
      ],
      "metadata": {
        "id": "5_4AjwBaM9ij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 평가함수 (성능 측정을 위한)\n",
        "def evaluate(model, val_iter):\n",
        "    model.eval()\n",
        "    corrects, total_loss = 0, 0\n",
        "    for batch in val_iter:\n",
        "        x, y = batch.text.to(DEVICE), batch.label.to(DEVICE)\n",
        "        y.data.sub_(1)\n",
        "        # 기울기 계산 안함 (optimizer 있을 필요 X)\n",
        "        # x를 모델에 입력해서 예측값인 logit 계산\n",
        "        logit = model(x) # logit = 0과 1 사이의 확률\n",
        "        # 손실 함수 -> 손실\n",
        "        loss = F.cross_entropy(logit, y, reduction='sum') # 오차의 합\n",
        "        total_loss += loss.item()\n",
        "        # 예측값과 정답값이 일치하는 경우\n",
        "        # tensor.max(0) = 각 열에서의 최대값\n",
        "        # tensor.max(1) = 각 행에서의 최대값 / [1] -> 최대값의 인덱스 (0,1)\n",
        "        # max의 결과값 -> (최대값[0], 그 값의 인덱스(indices)[1])\n",
        "        # view -> y와 비교할 수 있게 같은 모양 변형 => 일치하는 건 True(1) 일치하지 않는 건 False(0)\n",
        "        # sum -> 일치하는 것들의 갯수만.\n",
        "        # https://velog.io/@jarvis_geun/torch.argmax-torch.max\n",
        "        corrects += (logit.max(1)[1].view(y.size()).data == y.data).sum()\n",
        "    size = len(val_iter.dataset) # 전체 데이터의 갯수\n",
        "    avg_loss = total_loss / size # 오차 평균\n",
        "    avg_accuracy = corrects / size * 100 # (%) - 정확도 평균\n",
        "    return avg_loss, avg_accuracy"
      ],
      "metadata": {
        "id": "5mNyCizzW7sA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 객체 정의\n",
        "# 1 : 내부 GRU의 층 개수\n",
        "# 256 : 모델 내 은닉 벡터의 차원값\n",
        "# 128 : 임베딩(벡터화)된 토큰의 차원값\n",
        "# 0.5 : 드롭아웃 비중\n",
        "model = BasicGRU(1, 256, vocab_size, 128, n_classes, 0.5).to(DEVICE)"
      ],
      "metadata": {
        "id": "GWpX1Yn4ZBvO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 최적화함수\n",
        "# 최적화함수를 뭘 쓸지 모르면 Adam 써라!\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
      ],
      "metadata": {
        "id": "w05XhQQtZhg7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 학습\n",
        "best_val_loss = None # 검증 오차를 최소화\n",
        "for epoch in range(1, EPOCHS+1):\n",
        "    train(model, optimizer, train_iter)\n",
        "    val_loss, val_accuracy = evaluate(model, val_iter)\n",
        "\n",
        "    # print(\"[에포크: %d] 검증 오차:%5.2f | 검증 정확도:%5.2f\" % (epoch, val_loss, val_accuracy))\n",
        "    # https://ooyoung.tistory.com/87\n",
        "    print(f\"[에포크: {epoch}] 검증 오차:{val_loss:5.2f} | 검증 정확도:{val_accuracy:5.2f}\")\n",
        "\n",
        "    # 검증 오차가 가장 적은 최적의 모델\n",
        "    if not best_val_loss or val_loss < best_val_loss:\n",
        "        # not best_val_loss = None은 not으로 취급 = 아직 최적 검증 오차가 비어있거나\n",
        "        # val_loss < best_val_loss = 새롭게 구한 검증 오차가 기존에 최적 검증 오차보다 작으면\n",
        "        # => 새로운 모델의 오차가 더 작으면\n",
        "        if not os.path.isdir(\"snapshot\"): # 스냅샷 폴더 없으면 만들어주세요\n",
        "            os.makedirs(\"snapshot\") # snapshot 폴더에 저장할 것임\n",
        "        torch.save(model.state_dict(), './snapshot/txtclassification.pt')\n",
        "        best_val_loss = val_loss # 오차 갱신"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iv4lh_IQaHWV",
        "outputId": "2841f4d1-14e0-43eb-819b-8469590bfd87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[에포크: 1] 검증 오차: 0.69 | 검증 정확도:50.92\n",
            "[에포크: 2] 검증 오차: 0.69 | 검증 정확도:51.68\n",
            "[에포크: 3] 검증 오차: 0.70 | 검증 정확도:54.02\n",
            "[에포크: 4] 검증 오차: 0.52 | 검증 정확도:75.20\n",
            "[에포크: 5] 검증 오차: 0.37 | 검증 정확도:83.60\n",
            "[에포크: 6] 검증 오차: 0.36 | 검증 정확도:85.54\n",
            "[에포크: 7] 검증 오차: 0.39 | 검증 정확도:85.48\n",
            "[에포크: 8] 검증 오차: 0.44 | 검증 정확도:85.74\n",
            "[에포크: 9] 검증 오차: 0.54 | 검증 정확도:85.40\n",
            "[에포크: 10] 검증 오차: 0.55 | 검증 정확도:84.82\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load('./snapshot/txtclassification.pt'))\n",
        "test_loss, test_acc = evaluate(model, test_iter)\n",
        "print('테스트 오차: %5.2f | 테스트 정확도: %5.2f' % (test_loss, test_acc))"
      ],
      "metadata": {
        "id": "RFuhJ3yXeDUJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a103034-2b3d-4c43-fb7b-8e56c2bab46d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "테스트 오차:  0.35 | 테스트 정확도: 85.86\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 한글 : 네이버 영화 리뷰 감정 분석"
      ],
      "metadata": {
        "id": "-dURPQNrk9H2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://velog.io/@heiswicked/M1-Part11-%EB%B3%B5%EB%B6%88%EB%B3%B5%EC%84%A4%EC%B9%98-konlpy.tag-MECAB-on-M1-ver.221230\n",
        "!pip install konlpy\n",
        "!pip install mecab-python\n",
        "!bash <(curl -s https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w4nDoERBeEEt",
        "outputId": "dcd988fb-bf44-4800-bc45-19eae02d8d98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: konlpy in /usr/local/lib/python3.9/dist-packages (0.6.0)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.9/dist-packages (from konlpy) (4.9.2)\n",
            "Requirement already satisfied: JPype1>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from konlpy) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.9/dist-packages (from konlpy) (1.22.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from JPype1>=0.7.0->konlpy) (23.0)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting mecab-python\n",
            "  Downloading mecab-python-1.0.0.tar.gz (1.3 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting mecab-python3\n",
            "  Downloading mecab_python3-1.0.6-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (581 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m581.5/581.5 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: mecab-python\n",
            "  Building wheel for mecab-python (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mecab-python: filename=mecab_python-1.0.0-py3-none-any.whl size=1251 sha256=4f9343281464d45d16bf36bb77859aabff72fcf240007a93d1384c8c96049abe\n",
            "  Stored in directory: /root/.cache/pip/wheels/63/09/15/cc401a7f8d041043978f3f60e64f7d65014522e104b7c9d1f2\n",
            "Successfully built mecab-python\n",
            "Installing collected packages: mecab-python3, mecab-python\n",
            "Successfully installed mecab-python-1.0.0 mecab-python3-1.0.6\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mmecab-ko is already installed\n",
            "mecab-ko-dic is already installed\n",
            "mecab-python is already installed\n",
            "Done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# https://konlpy.org/ko/latest/index.html\n",
        "from konlpy.tag import Mecab\n",
        "import pandas as pd\n",
        "\n",
        "# https://github.com/e9t/nsmc/\n",
        "data_path = 'https://raw.githubusercontent.com/e9t/nsmc/master/'"
      ],
      "metadata": {
        "id": "FqIjo09xlGTY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Mecab() # 형태소 분석기를 통한 토큰화\n",
        "\n",
        "# 훈련 데이터셋 & 시험 데이터셋\n",
        "trainset = pd.read_csv(data_path + \"ratings_train.txt\", sep='\\t') # tab으로 구분되어 있기 때문에 \\t를 구분자로 해서 read\n",
        "testset = pd.read_csv(data_path + \"ratings_test.txt\", sep='\\t')"
      ],
      "metadata": {
        "id": "e_nqPLx3l1u4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainset.head()"
      ],
      "metadata": {
        "id": "OcnUdl3km1nQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainset.drop(columns=['id'], inplace=True)\n",
        "testset.drop(columns=['id'], inplace=True)"
      ],
      "metadata": {
        "id": "S2qSY-Gem7NP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainset.head()"
      ],
      "metadata": {
        "id": "GFZbEGS3nBHC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainset.info()"
      ],
      "metadata": {
        "id": "0f6Nx9CvnXjT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "# 1. 결측치 (nan) 제거\n",
        "# 2. 훈련셋에서 30% -> 검증셋으로 분리\n",
        "train_data = trainset.dropna() # 말뭉치에서 nan 값을 제거\n",
        "test_data = testset.dropna()\n",
        "train_data, valid_data = train_test_split(train_data, test_size=0.3, random_state=71)"
      ],
      "metadata": {
        "id": "YWoqBgR_nLYJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TEXT, Label\n",
        "TEXT = data.Field(sequential=True, use_vocab=True, tokenize=tokenizer.morphs,\n",
        "                  lower=False, batch_first=True, fix_length=20) # 모든 품사 (모든 단어)\n",
        "# TEXT = data.Field(sequential=True, use_vocab=True, tokenize=tokenizer.nouns,\n",
        "#                   lower=False, batch_first=True) # 명사만\n",
        "# LABEL = data.LabelField(batch_first=True, dtype=torch.float)\n",
        "LABEL = data.Field(sequential=False, batch_first=True)"
      ],
      "metadata": {
        "id": "d_qJgkUMntK2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 파이토치용 (텍스트) 데이터셋으로 변환\n",
        "# input_data = pandas df => iterrows() => row (document, label) => TEXT, LABEL\n",
        "def convert_dataset(input_data, text, label):\n",
        "    list_of_example = [data.Example.fromlist(row.tolist(),\n",
        "                                             fields=[('text', text), ('label', label)])\n",
        "                        for _, row in input_data.iterrows()]\n",
        "    dataset = data.Dataset(examples=list_of_example, fields=[('text', text), ('label', label)])\n",
        "    return dataset"
      ],
      "metadata": {
        "id": "gtA7bE93oqMv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = convert_dataset(train_data, TEXT, LABEL)\n",
        "valid_data = convert_dataset(valid_data, TEXT, LABEL)\n",
        "test_data = convert_dataset(test_data, TEXT, LABEL)"
      ],
      "metadata": {
        "id": "07KZ2pUHpbiu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.examples[0].text, train_data.examples[0].label"
      ],
      "metadata": {
        "id": "pfvtMVE9tJq1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.examples[0].text, train_data.examples[0].label"
      ],
      "metadata": {
        "id": "nOaBM6cKpwlm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.examples[10].text, train_data.examples[10].label"
      ],
      "metadata": {
        "id": "ZOH3VCnHqD2T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 단어 사전\n",
        "TEXT.build_vocab(train_data, max_size=10000)\n",
        "# TEXT.build_vocab(train_data, min_freq=5)\n",
        "LABEL.build_vocab(train_data)"
      ],
      "metadata": {
        "id": "8HG2-B7VqKo4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TEXT.vocab.freqs.most_common(10)"
      ],
      "metadata": {
        "id": "hgtHACJbqU3d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# BATCH_SIZE = 64\n",
        "# 학습에 사용될 iter 정의\n",
        "train_iter, val_iter, test_iter = data.Iterator.splits(\n",
        "    (train_data, valid_data, test_data),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True, repeat=False, sort=False, device=DEVICE\n",
        ")"
      ],
      "metadata": {
        "id": "_BjFEJmAqlvA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# vocab_size\n",
        "vocab_size = len(TEXT.vocab)"
      ],
      "metadata": {
        "id": "JwJY5dx8rFim"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size"
      ],
      "metadata": {
        "id": "9M-Y62norOYM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# n_classes = 2\n",
        "model = BasicGRU(1, 256, vocab_size, 128, n_classes, 0.5).to(DEVICE)\n",
        "# model = BasicGRU(2, 256, vocab_size, 128, n_classes, 0.5).to(DEVICE)"
      ],
      "metadata": {
        "id": "8MPMsuSTrRJO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr = 0.001\n",
        "# lr = 0.0001\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
      ],
      "metadata": {
        "id": "LIPtY-97rdgN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 학습\n",
        "EPOCHS = 10\n",
        "# EPOCHS = 20\n",
        "best_val_loss = None # 검증 오차를 최소화\n",
        "for epoch in range(1, EPOCHS+1):\n",
        "    train(model, optimizer, train_iter)\n",
        "    val_loss, val_accuracy = evaluate(model, val_iter)\n",
        "\n",
        "    print(f\"[에포크: {epoch}] 검증 오차:{val_loss:5.2f} | 검증 정확도:{val_accuracy:5.2f}\")\n",
        "\n",
        "    if not best_val_loss or val_loss < best_val_loss:\n",
        "        if not os.path.isdir(\"snapshot\"): \n",
        "            os.makedirs(\"snapshot\")\n",
        "        torch.save(model.state_dict(), './snapshot/txtclassification_ko.pt')\n",
        "        best_val_loss = val_loss"
      ],
      "metadata": {
        "id": "c4TwgwP9rlPM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load('./snapshot/txtclassification_ko.pt'))\n",
        "test_loss, test_acc = evaluate(model, test_iter)\n",
        "print('테스트 오차: %5.2f | 테스트 정확도: %5.2f' % (test_loss, test_acc))"
      ],
      "metadata": {
        "id": "AzAMutbAr9zt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}